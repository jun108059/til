## 회귀 분석으로 연속적 타깃 변수 예측


지도 학습의 또 다른 하위 카테고리인 회귀 분석(regression analysis)을 알아보자

목차
10.1 선형 회귀
10.2 주택 데이터셋 탐색
10.3 최소 제곱 선형 회귀 모델 구현
10.4 RANSAC을 사용하여 안정된 회귀 모델 훈련
10.5 선형 회귀 모델의 성능 평가
10.6 회귀에 규제 적용
10.7 선형 회귀 모델을 다항 회귀로 변환
10.8 랜덤 포레스트를 사용하여 비선형 관계 다루기
10.9 요약
회귀 모델

연속적인 타깃 변수를 예측하는 데 사용되기 떄문에 산업 현장의 애플리케이션은 물론 과학 문제를 해결하기 위한 도구

 

회귀 모델의 주요 개념과 다음 주제에 대해 알아본다.

- 데이터 탐색과 시각화하기
- 선형 회귀 모델을 구현하는 여러 가지 방법 알아보기
- 이상치에 민감하지 않은 회귀 모델 훈련하기
- 회귀 모델을 평가하고 문제점 분석하기
- 비선형 데이터에 회귀 모델 학습하기

## 10.1 선형 회귀

선형 회귀

하나 이상의 특성과 연속적인 타깃 변수 사이의 관계를 모델링하는 것이 목적

 
지도 학습은 분류 알고리즘과 회귀 분석의 카테고리로 분류된다.

분류 알고리즘 : 범주형 클래스 레이블을 예측

회귀 분석 : 연속적인 출력 값을 예측

 

### 10.1.1 단변량 선형 회귀

단변량 선형 회귀(univariate linear regression)는 가장 기본적인 선형 회귀이다.

하나의 특성과 연속적인 타깃 사이의 관계를 모델링한다.

 

하나의 특성 : 설명 변수(explanatory variable) - x
연속적인 타깃 : 응답 변수(response variable) - y
 


선형 모델 공식
여기서 w0는 y축 절편, w1은 특성의 가중치이다.

특성과 타깃 사이의 관계를 나타내는 선형 방정식의 가중치를 학습하는 것이 목적이다.

이 방정식으로 훈련 데이터셋이 아닌 새로운 샘플의 타깃 값을 예측할 수 있다!

 


선형 회귀 그래프
데이터에 가장 잘 맞는 이런 직선을 회귀 직선(regression line)이라고 한다!

회귀 직선과 샘플 포인트 사이의 직선 거리를 오프셋(offset) 또는 예측 오차인 잔차(residual)라고 합니다.

 

 

### 10.1.2 다변량 선형 회귀
여러 개의 특성이 있는 경우로 일반화 한 경우!


다변량 선형 회귀
여기서 w0는 y축의 절편이고 x0=1 이다.

 


다변량 회귀 모델 가시화
두 개의 특성을 가진 다변량 회귀 모델이 학습한 2차원 초평면으로 가시화했다.

 

여기서 보듯이 정적인 이미지로 그려진 다변량 회귀 모델의 그래프는 3차원 산점도만 되어도 이해하기 어렵다.

 

## 10.2 주택 데이터셋 탐색
첫 번째 선형 회귀 모델을 만들기 전에 새로운 주택 데이터셋을 받아 간단한 그래프를 그려 데이터를 탐색하며 이해해보자.

 

이 데이터셋은 1978년에 해리슨과 루빈펠드가 수집한 보스턴 교외 지역의 주택 정보를 담고있다.
 

### 10.2.1 데이터프레임으로 주택 데이터셋 읽기
판다스의 read_csv함수로 읽어들이자.

 

read_csv 함수
빠르고 많은 기능이 있기 때문에 텍스트로 저장된 csv 형태의 파일을 다룰 때 좋다!

1. CRIM: 도시의 인당 범죄율
2. ZN: 25,000 평방 피트가 넘는 주택 비율
3. INDUS: 도시에서 소매 업종이 아닌 지역 비율
4. CHAS: 찰스강 인접 여부(강 주변=1, 그 외=0)
5. NOX: 일산화질소 농도(10ppm 당)
6. RM: 주택의 평균 방 개수
7. AGE: 1940년 이전에 지어진 자가 주택 비율
8. DIS: 다섯 개의 보스턴 고용 센터까지 가중치가 적용된 거리
9. RAD: 방사형으로 뻗은 고속도로까지 접근성 지수
10. TAX: $10,000당 재산세율
11. PTRATIO: 도시의 학생-교사 비율
12. B: 1000(Bk - 0.63)^2, 여기에서 Bk는 도시의 아프리카계 미국인 비율
13. LSTAT: 저소득 계층의 비율
14. MEDV: 자가 주택의 중간 가격($1,000 단위)

나머지 부분에서는 주택 가격(MEDV)을 타깃 값으로 삼겠다.

 

13개의 특성 중 하나 이상을 사용하여 이 값을 예측해보자.

먼저 깃허브에서 판다스 DataFrame으로 데이터를 불러왔다.


깃허브에서 판다스 DataFrame으로 데이터를 로딩

### 10.2.2 데이터셋의 중요 특징 시각화

탐색적 데이터 분석(Exploratory Data Analysis, EDA)은 머신 러닝 모델을 훈련하기 전에 첫 번째로 수행할 중요하고 권장되는 단계이다.

이상치를 감지하고 데이터 분포를 시각화하거나 특성 간의 관계를 나타내는 데 도움이 된다.
먼저 산점도 행렬(scatterplot matrix)을 그려서 데이터셋에 있는 특성 간의 상관관계를 한 번에 시각화해보자.

> Seaborn 라이브러리  
> Seaborn라이브러리의 pairplot 함수를 사용하면 산점도 행렬을 그릴 수 있다.  
> 통계 그래프를 그릴 수 있는 맷플롯립 기반의 파이썬 라이브러리이다!

seaborn 패키지를 import 받아 산점도 행렬을 만든다.
산점도 행렬은 데이터셋에 있는 특성 간의 관계를 시각적으로 잘 요약해준다!

다섯 개의 열만 산점도 행렬에 포함시킨 결과이다.

산점도 행렬
데이터셋의 다른 부분을 포함시켜 산점도 행렬을 그려주면 데이터가 어떻게 분포되어 있는지, 이상치를 포함하고 있는지 빠르게 확인할 수 있다!
 

### 10.2.3 상관관계 행렬을 사용한 분석
산관관계 행렬(correlation matrix)

주택 데이터셋 특성들을 변수 간의 선형 관계를 정량화하고 요약해보자.

주성분 분석의 공분산 행렬과 밀접한 관련이 있음!
상관관계 행렬 : 스케일 조정된 공분산 행렬
특성이 표준화되어 있으면 상관관계 행렬과 공분산 행렬이 같다!
피어슨의 상관관계 계수를 포함하고 있는 정방 행렬이다.
피어슨의 상관관계 계수는 특성 사이의 선형 의존성을 측정한다!

상관관계 계수의 범위 = -1 ~ 1
r=1 두 특성이 완벽한 양의 상관관계
r=0 아무런 상관관계가 없다
r=-1 완벽한 음의 상관관계

단순히 두 특성 x와 y사이의 공분산(분자)을 표준 편차의 곱(분모)로 나눈 것
앞서 산점도 행렬로 그렸던 다섯 개의 특성에 넘파이 corrcoef 함수를 사용해보자.


상관관계 행렬의 히트맵
결과 그래프를 보면 상관관계 행렬은 선형 상관관계를 바탕으로 특성을 선택하는 데 유용한 정보를 요약해준다.

 

우리는 훈련하기 위해 타깃 변수 MEDV와 상관관계가 높은 특성을 선택하는 것이 좋다!

따라서 LSTAT(-0.74) 또는 RM(0.70)을 선택하는 것이 좋은데, 산점도 행렬에서 보았을 때 선형 관계를 갖는 RM을 선택하면 좋다!

 

## 10.3 최소 제곱 선형 회귀 모델 구현  

선형 회귀는 훈련 데이터의 샘플 포인터에 가장 잘 맞는 직선을 찾는 것!

이때 가장 잘 맞다의 정의, 그런 모델을 학습할 수 있는 방법에 최소 제곱법(OLS)이 이용된다.

최소 제곱법(OLS)?
샘플 포인트까지 수직 거리의 제곱 합(오차 또는 잔차)을 최소화하는 선형 회귀 직선의 모델 파라미터를 추정하는 방법

### 10.3.1 경사 하강법으로 회귀 모델의 파라미터 구하기
앞 포스팅에서 선형 활성화 함수를 사용한 인공 뉴런인 아달린(Adaline)을 구현했었고, 비용 함수 J를 정의하고 경사 하강법(GD)과 확률적 경사 하강법(SGD) 같은 최적화 알고리즘을 사용해 이 함수를 최소화하는 가중치를 학습했다.
아달린의 비용 함수는 제곱 오차합으로 OLS에서 사용할 비용 함수와 같다!

OSL에서 사용할 비용 함수
아달린의 경사 하강법 구현에서 단위 계단 함수를 제거하면 첫 번째 선형 회귀 모델을 구현해보자.


단위 함수를 제거한 첫 번째 선형 회귀 모델을 구현
모델을 실행하기 위해 RM(방 개수) 변수를 특성으로 사용하여 MEDV(주택 가격)을 예측하는 모델을 훈련시키면


RM과 MEDV를 예측하는 모델을 훈련 (특성 표준화 전처리 포함)
경사 하강법 같은 최적화 알고리즘을 사용할 때 훈련 데이터셋을 반복하는 에포크의 함수로 비용을 그래프로 그려보는 것이 좋다!



출처: https://dev-youngjun.tistory.com/27 [개발하는만두 ^___^]
